"""
Script d'Exploration des Donn√©es - Pr√©sentation Projet P10
===========================================================

Ce script explore et analyse le dataset Globo.com pour d√©montrer :
1. La qualit√© et la composition du dataset
2. Les analyses pr√©liminaires effectu√©es
3. Le processus de filtrage et pr√©paration
4. Les insights d√©couverts

Utilisation pour la pr√©sentation/soutenance.

Auteur : Projet P10 - Syst√®me de Recommandation
Date : 18 D√©cembre 2024
"""

import pandas as pd
import numpy as np
import json
import pickle
from pathlib import Path
from datetime import datetime

def print_section(title):
    """Affiche un titre de section format√©"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80 + "\n")


def explore_dataset_overview():
    """Vue d'ensemble du dataset Globo.com"""
    print_section("1. VUE D'ENSEMBLE DU DATASET GLOBO.COM")

    # Charger metadata
    df_meta = pd.read_csv('models/articles_metadata.csv')

    # Charger user profiles
    with open('models/user_profiles.json', 'r') as f:
        user_profiles = json.load(f)

    # Charger matrice
    from scipy.sparse import load_npz
    user_item_matrix = load_npz('models/user_item_matrix.npz')

    print("üìä STATISTIQUES G√âN√âRALES")
    print(f"   Articles totaux dans la base : {len(df_meta):,}")
    print(f"   Utilisateurs totaux : {len(user_profiles):,}")
    print(f"   Interactions totales : {user_item_matrix.nnz:,}")
    print(f"   P√©riode couverte : 3 mois (dataset acad√©mique)")

    # Sparsit√©
    density = user_item_matrix.nnz / (user_item_matrix.shape[0] * user_item_matrix.shape[1]) * 100
    print(f"\n   Sparsit√© de la matrice : {100-density:.4f}%")
    print(f"   Densit√© : {density:.4f}%")

    # Moyennes
    avg_interactions_per_user = user_item_matrix.nnz / user_item_matrix.shape[0]
    avg_interactions_per_article = user_item_matrix.nnz / user_item_matrix.shape[1]

    print(f"\n   Interactions moyennes par utilisateur : {avg_interactions_per_user:.1f}")
    print(f"   Interactions moyennes par article : {avg_interactions_per_article:.1f}")

    return df_meta, user_profiles, user_item_matrix


def analyze_article_quality(df_meta):
    """Analyse de la qualit√© des articles"""
    print_section("2. ANALYSE DE LA QUALIT√â DES ARTICLES")

    print("üìù DISTRIBUTION DU NOMBRE DE MOTS")
    print(f"   Minimum : {df_meta['words_count'].min()} mots")
    print(f"   Maximum : {df_meta['words_count'].max():,} mots")
    print(f"   Moyenne : {df_meta['words_count'].mean():.1f} mots")
    print(f"   M√©diane : {df_meta['words_count'].median():.1f} mots")
    print(f"   √âcart-type : {df_meta['words_count'].std():.1f} mots")

    # Cat√©gorisation
    zero_words = df_meta[df_meta['words_count'] == 0]
    short_articles = df_meta[(df_meta['words_count'] > 0) & (df_meta['words_count'] < 50)]
    normal_articles = df_meta[(df_meta['words_count'] >= 50) & (df_meta['words_count'] < 500)]
    long_articles = df_meta[df_meta['words_count'] >= 500]

    print(f"\nüîç SEGMENTATION PAR LONGUEUR")
    print(f"   Articles vides (0 mots) : {len(zero_words):,} ({len(zero_words)/len(df_meta)*100:.2f}%)")
    print(f"      ‚Üí Probablement erreurs de crawling")

    print(f"\n   Articles courts (1-49 mots) : {len(short_articles):,} ({len(short_articles)/len(df_meta)*100:.2f}%)")
    print(f"      ‚Üí Br√®ves, flash info, scores sportifs")

    print(f"\n   Articles normaux (50-499 mots) : {len(normal_articles):,} ({len(normal_articles)/len(df_meta)*100:.2f}%)")
    print(f"      ‚Üí Contenu √©ditorial standard")

    print(f"\n   Articles longs (‚â•500 mots) : {len(long_articles):,} ({len(long_articles)/len(df_meta)*100:.2f}%)")
    print(f"      ‚Üí Analyses approfondies, enqu√™tes")

    # Qualit√© globale
    quality_articles = df_meta[df_meta['words_count'] >= 50]
    print(f"\n‚úÖ QUALIT√â GLOBALE")
    print(f"   Articles de qualit√© (‚â•50 mots) : {len(quality_articles):,}")
    print(f"   Taux de qualit√© : {len(quality_articles)/len(df_meta)*100:.2f}%")

    return quality_articles


def analyze_categories(df_meta):
    """Analyse de la distribution des cat√©gories"""
    print_section("3. DISTRIBUTION DES CAT√âGORIES")

    n_categories = df_meta['category_id'].nunique()
    print(f"üìö Nombre de cat√©gories uniques : {n_categories}")

    # Top cat√©gories
    top_categories = df_meta['category_id'].value_counts().head(15)

    print(f"\nüèÜ TOP 15 CAT√âGORIES (par nombre d'articles)")
    print(f"\n   {'Cat√©gorie ID':<15} {'Nb Articles':<15} {'Pourcentage'}")
    print(f"   {'-'*50}")

    for cat_id, count in top_categories.items():
        pct = count / len(df_meta) * 100
        print(f"   {cat_id:<15} {count:<15,} {pct:>6.2f}%")

    # Concentration
    top_10_pct = top_categories.head(10).sum() / len(df_meta) * 100
    print(f"\n   Les 10 cat√©gories principales repr√©sentent {top_10_pct:.1f}% du contenu")

    return top_categories


def analyze_user_activity(user_profiles, user_item_matrix):
    """Analyse de l'activit√© utilisateur"""
    print_section("4. ANALYSE DE L'ACTIVIT√â UTILISATEUR")

    # Distribution du nombre d'interactions
    interactions_per_user = []
    for user_id, profile in user_profiles.items():
        if isinstance(profile, dict) and 'articles_read' in profile:
            interactions_per_user.append(len(profile['articles_read']))
        elif isinstance(profile, list):
            interactions_per_user.append(len(profile))

    interactions_per_user = np.array(interactions_per_user)

    print("üë• DISTRIBUTION DES INTERACTIONS PAR UTILISATEUR")
    print(f"   Minimum : {interactions_per_user.min()} interactions")
    print(f"   Maximum : {interactions_per_user.max()} interactions")
    print(f"   Moyenne : {interactions_per_user.mean():.1f} interactions")
    print(f"   M√©diane : {np.median(interactions_per_user):.1f} interactions")
    print(f"   √âcart-type : {interactions_per_user.std():.1f}")

    # Segmentation utilisateurs
    print(f"\nüéØ SEGMENTATION DES UTILISATEURS")

    segments = [
        ("Tr√®s passifs (1 clic)", 1, 1),
        ("Passifs (2-4 clics)", 2, 4),
        ("Occasionnels (5-10 clics)", 5, 10),
        ("R√©guliers (11-20 clics)", 11, 20),
        ("Actifs (21-50 clics)", 21, 50),
        ("Tr√®s actifs (51+ clics)", 51, 10000)
    ]

    total_users = len(interactions_per_user)

    for segment_name, min_val, max_val in segments:
        count = np.sum((interactions_per_user >= min_val) & (interactions_per_user <= max_val))
        pct = count / total_users * 100
        print(f"   {segment_name:<30} : {count:>7,} ({pct:>5.1f}%)")

    # Impact pour benchmark
    eligible_for_benchmark = np.sum(interactions_per_user >= 5)
    pct_eligible = eligible_for_benchmark / total_users * 100

    print(f"\n‚ö†Ô∏è  IMPACT POUR L'√âVALUATION")
    print(f"   Users √©ligibles (‚â•5 interactions) : {eligible_for_benchmark:,} ({pct_eligible:.1f}%)")
    print(f"   Users exclus (<5 interactions) : {total_users - eligible_for_benchmark:,} ({100-pct_eligible:.1f}%)")
    print(f"\n   ‚Üí Les m√©triques sont calcul√©es uniquement sur les {pct_eligible:.1f}% d'users actifs")

    return interactions_per_user


def analyze_temporal_distribution(df_meta):
    """Analyse de la distribution temporelle"""
    print_section("5. ANALYSE TEMPORELLE DES ARTICLES")

    # Convertir timestamps
    df_meta['created_date'] = pd.to_datetime(df_meta['created_at_ts'], unit='ms')
    df_meta['year_month'] = df_meta['created_date'].dt.to_period('M')

    # Distribution par mois
    monthly_dist = df_meta['year_month'].value_counts().sort_index()

    print("üìÖ DISTRIBUTION PAR P√âRIODE")
    print(f"   Date la plus ancienne : {df_meta['created_date'].min()}")
    print(f"   Date la plus r√©cente : {df_meta['created_date'].max()}")
    print(f"   P√©riode couverte : {(df_meta['created_date'].max() - df_meta['created_date'].min()).days} jours")

    print(f"\n   Articles par mois (derniers 12 mois) :")
    for period, count in monthly_dist.tail(12).items():
        print(f"      {period} : {count:>6,} articles")

    # Fra√Æcheur moyenne
    now = pd.Timestamp.now()
    df_meta['age_days'] = (now - df_meta['created_date']).dt.days

    print(f"\n‚è∞ FRA√éCHEUR DES ARTICLES")
    print(f"   √Çge moyen : {df_meta['age_days'].mean():.0f} jours")
    print(f"   √Çge m√©dian : {df_meta['age_days'].median():.0f} jours")

    fresh = df_meta[df_meta['age_days'] <= 7]
    print(f"\n   Articles frais (‚â§7 jours) : {len(fresh):,} ({len(fresh)/len(df_meta)*100:.1f}%)")

    return df_meta


def analyze_data_used_in_system():
    """Analyse des donn√©es effectivement utilis√©es par le syst√®me"""
    print_section("6. DONN√âES UTILIS√âES PAR LE SYST√àME")

    # Charger metadata
    df_meta = pd.read_csv('models/articles_metadata.csv')

    # Charger user profiles
    with open('models/user_profiles.json', 'r') as f:
        user_profiles = json.load(f)

    # Extraire articles utilis√©s
    all_articles_used = set()
    for user_id, profile in user_profiles.items():
        if isinstance(profile, dict) and 'articles_read' in profile:
            all_articles_used.update(profile['articles_read'])
        elif isinstance(profile, list):
            all_articles_used.update(profile)

    print(f"üéØ FILTRAGE DES DONN√âES")
    print(f"   Articles totaux dans dataset : {len(df_meta):,}")
    print(f"   Articles avec ‚â•1 interaction : {len(all_articles_used):,}")
    print(f"   Taux de couverture : {len(all_articles_used)/len(df_meta)*100:.1f}%")

    # Analyser qualit√© des articles utilis√©s
    df_used = df_meta[df_meta['article_id'].isin(all_articles_used)]

    zero_words = df_used[df_used['words_count'] == 0]
    short = df_used[df_used['words_count'] < 50]
    normal = df_used[df_used['words_count'] >= 50]

    print(f"\n‚úÖ QUALIT√â DES ARTICLES UTILIS√âS")
    print(f"   Articles vides (0 mots) : {len(zero_words)} ({len(zero_words)/len(df_used)*100:.3f}%)")
    print(f"   Articles courts (<50 mots) : {len(short)} ({len(short)/len(df_used)*100:.2f}%)")
    print(f"   Articles normaux (‚â•50 mots) : {len(normal)} ({len(normal)/len(df_used)*100:.2f}%)")

    print(f"\n   ‚Üí Le syst√®me recommande {len(normal)/len(df_used)*100:.1f}% de contenu √©ditorial de qualit√©")

    return df_used


def analyze_cold_start_problem():
    """Analyse du probl√®me cold-start"""
    print_section("7. ANALYSE DU PROBL√àME COLD-START")

    # Charger user profiles
    with open('models/user_profiles.json', 'r') as f:
        user_profiles = json.load(f)

    # Distribution
    interactions_count = []
    for user_id, profile in user_profiles.items():
        if isinstance(profile, dict) and 'articles_read' in profile:
            interactions_count.append(len(profile['articles_read']))
        elif isinstance(profile, list):
            interactions_count.append(len(profile))

    interactions_count = np.array(interactions_count)

    print("üÜï PROBL√àME DU COLD-START")

    # Nouveaux utilisateurs
    new_users = np.sum(interactions_count < 5)
    experienced_users = np.sum(interactions_count >= 5)

    total = len(interactions_count)

    print(f"\n   Utilisateurs 'cold-start' (<5 interactions) : {new_users:,} ({new_users/total*100:.1f}%)")
    print(f"   Utilisateurs 'warm-start' (‚â•5 interactions) : {experienced_users:,} ({experienced_users/total*100:.1f}%)")

    print(f"\nüìä STRAT√âGIE DE RECOMMANDATION PAR SEGMENT")
    print(f"\n   Cold-start users ({new_users/total*100:.0f}%) :")
    print(f"      ‚Ä¢ Pas assez d'historique pour collaborative/content-based")
    print(f"      ‚Ä¢ Solution : Popularit√© + Temporal Decay")
    print(f"      ‚Ä¢ Performance estim√©e : 2-3% HR@5")

    print(f"\n   Warm-start users ({experienced_users/total*100:.0f}%) :")
    print(f"      ‚Ä¢ Historique suffisant pour personnalisation")
    print(f"      ‚Ä¢ Solution : Hybride (Collab + Content + Trend)")
    print(f"      ‚Ä¢ Performance mesur√©e : 7.0% HR@5")

    print(f"\n   Performance globale estim√©e :")
    cold_start_hr = 0.02
    warm_start_hr = 0.07
    global_hr = (new_users/total * cold_start_hr + experienced_users/total * warm_start_hr) * 100
    print(f"      {new_users/total*100:.0f}% √ó 2% + {experienced_users/total*100:.0f}% √ó 7% = {global_hr:.1f}% HR@5")


def generate_summary_report():
    """G√©n√®re un rapport de synth√®se"""
    print_section("8. SYNTH√àSE DE L'EXPLORATION")

    print("üìã D√âCISIONS DE PR√âPARATION DES DONN√âES\n")

    decisions = [
        ("Filtrage des articles vides", "Exclusion des 35 articles avec 0 mots (0.01%)", "‚úÖ Appliqu√©"),
        ("Conservation des br√®ves", "Inclusion des articles courts (1-49 mots) car contenu √©ditorial", "‚úÖ Appliqu√©"),
        ("Filtrage users cold-start", "Exclusion des users <5 interactions pour benchmark", "‚úÖ Appliqu√©"),
        ("Conservation pour production", "Users cold-start servis par popularit√©", "‚úÖ Planifi√©"),
        ("Sparse matrix", "CSR format pour g√©rer 99.96% de sparsit√©", "‚úÖ Appliqu√©"),
        ("Temporal decay", "Half-life 7 jours pour news fra√Æcheur", "‚úÖ Appliqu√©"),
        ("Interaction weighting", "Poids 0.29-0.81 bas√©s sur engagement", "‚úÖ Appliqu√©"),
    ]

    for i, (decision, rationale, status) in enumerate(decisions, 1):
        print(f"   {i}. {decision}")
        print(f"      Rationale : {rationale}")
        print(f"      Status : {status}\n")

    print("\nüéØ R√âSULTATS CL√âS DE L'EXPLORATION\n")

    insights = [
        "99% du contenu est de qualit√© (‚â•50 mots)",
        "56% des users sont 'cold-start' (<5 interactions)",
        "Sparsit√© tr√®s √©lev√©e (99.96%) ‚Üí Sparse matrices obligatoires",
        "Dataset propre : pas de pages syst√®me (mentions l√©gales, etc.)",
        "Diversit√© : 400+ cat√©gories disponibles",
        "Fra√Æcheur importante pour news ‚Üí Temporal decay essentiel",
    ]

    for i, insight in enumerate(insights, 1):
        print(f"   {i}. {insight}")

    print("\n" + "=" * 80)
    print("  Fin de l'exploration - Donn√©es pr√™tes pour le preprocessing")
    print("=" * 80)


def main():
    """Point d'entr√©e principal"""
    print("\n")
    print("‚ïî" + "=" * 78 + "‚ïó")
    print("‚ïë" + " " * 20 + "EXPLORATION DU DATASET GLOBO.COM" + " " * 26 + "‚ïë")
    print("‚ïë" + " " * 15 + "Syst√®me de Recommandation d'Articles de News" + " " * 19 + "‚ïë")
    print("‚ïö" + "=" * 78 + "‚ïù")

    try:
        # 1. Vue d'ensemble
        df_meta, user_profiles, user_item_matrix = explore_dataset_overview()

        # 2. Qualit√© des articles
        quality_articles = analyze_article_quality(df_meta)

        # 3. Cat√©gories
        top_categories = analyze_categories(df_meta)

        # 4. Activit√© utilisateur
        interactions_dist = analyze_user_activity(user_profiles, user_item_matrix)

        # 5. Distribution temporelle
        df_meta_enriched = analyze_temporal_distribution(df_meta)

        # 6. Donn√©es utilis√©es
        df_used = analyze_data_used_in_system()

        # 7. Cold-start
        analyze_cold_start_problem()

        # 8. Synth√®se
        generate_summary_report()

        print("\n‚úÖ Exploration termin√©e avec succ√®s !\n")

    except FileNotFoundError as e:
        print(f"\n‚ùå Erreur : Fichier non trouv√© - {e}")
        print("   Assurez-vous que le preprocessing a √©t√© ex√©cut√©.")
    except Exception as e:
        print(f"\n‚ùå Erreur inattendue : {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
